{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33819b69-f99f-40a8-8a2b-7e14ab383ef9",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Generate two random samples\n",
    "sample1 = np.random.normal(loc=0, scale=1, size=100)\n",
    "sample2 = np.random.normal(loc=0.5, scale=1, size=100)\n",
    "\n",
    "# Perform two-sample t-test\n",
    "t_statistic, p_value = stats.ttest_ind(sample1, sample2)\n",
    "\n",
    "# Print the results\n",
    "print(\"T-statistic:\", t_statistic)\n",
    "print(\"P-value:\", p_value)\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is significant evidence that the population means are different.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is not enough evidence to conclude that the population means are different.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35a4b9b-b7e9-4d4d-aced-ac3e10148b32",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Generate two random samples\n",
    "sample1 = np.random.normal(loc=0, scale=1, size=100)\n",
    "sample2 = np.random.normal(loc=0.5, scale=1, size=100)\n",
    "\n",
    "# Perform two-sample t-test\n",
    "t_statistic, p_value = stats.ttest_ind(sample1, sample2)\n",
    "\n",
    "# Print the results\n",
    "print(\"T-statistic:\", t_statistic)\n",
    "print(\"P-value:\", p_value)\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is significant evidence that the population means are different.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is not enough evidence to conclude that the population means are different.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce1a615-c141-4795-ac2e-f6c4741ce9e4",
   "metadata": {},
   "source": [
    "The partitioning of variance in ANOVA refers to the division of the total variance in the data into different components that can be attributed to different sources or factors. This partitioning allows us to understand how much of the variability in the dependent variable can be explained by the independent variables (factors) included in the analysis.\n",
    "\n",
    "There are typically three main components of variance in ANOVA:\n",
    "\n",
    "1. Between-group variance: This component represents the variability between the group means. It reflects the extent to which the groups differ from each other on the dependent variable. In ANOVA terms, this is often referred to as the \"treatment effect\" or \"factor effect.\"\n",
    "\n",
    "2. Within-group variance: Also known as error variance or residual variance, this component represents the variability within each group that cannot be explained by the factors included in the analysis. It includes random variability as well as any other sources of variability not accounted for by the independent variables.\n",
    "\n",
    "3. Total variance: This is the overall variability in the dependent variable across all observations. It is the sum of the between-group variance and the within-group variance.\n",
    "\n",
    "Understanding the partitioning of variance is important for several reasons:\n",
    "\n",
    "- It allows us to assess the relative importance of different factors or treatments in explaining the variability in the dependent variable.\n",
    "- It helps in determining the statistical significance of the factors included in the analysis by comparing the between-group variance to the within-group variance.\n",
    "- It provides insights into the proportion of variance that remains unexplained by the factors included in the analysis, which can inform future research or adjustments to the model.\n",
    "- It aids in interpretation by quantifying the extent to which different sources of variability contribute to the overall variability in the data.\n",
    "\n",
    "Overall, understanding the partitioning of variance in ANOVA is essential for making valid inferences about the relationships between independent and dependent variables and for drawing meaningful conclusions from the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1228dcb-f14f-4d05-bc2e-b59b4c57c2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sum of Squares (SST): 1087.3333333333333\n",
      "Explained Sum of Squares (SSE): 1000.5333333333334\n",
      "Residual Sum of Squares (SSR): 86.79999999999984\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example data (replace with your data)\n",
    "group1 = np.array([10, 12, 15, 8, 11])\n",
    "group2 = np.array([20, 18, 25, 21, 24])\n",
    "group3 = np.array([30, 32, 35, 28, 31])\n",
    "\n",
    "# Combine all data into one array\n",
    "all_data = np.concatenate([group1, group2, group3])\n",
    "\n",
    "# Calculate overall mean\n",
    "overall_mean = np.mean(all_data)\n",
    "\n",
    "# Calculate total sum of squares (SST)\n",
    "SST = np.sum((all_data - overall_mean)**2)\n",
    "\n",
    "# Calculate group means\n",
    "group_means = np.array([np.mean(group1), np.mean(group2), np.mean(group3)])\n",
    "\n",
    "# Calculate explained sum of squares (SSE)\n",
    "SSE = np.sum((group_means - overall_mean)**2 * len(group1))\n",
    "\n",
    "# Calculate residual sum of squares (SSR)\n",
    "SSR = SST - SSE\n",
    "\n",
    "print(\"Total Sum of Squares (SST):\", SST)\n",
    "print(\"Explained Sum of Squares (SSE):\", SSE)\n",
    "print(\"Residual Sum of Squares (SSR):\", SSR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94177747-9a28-46a1-a0f5-3d443aa394d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Create a DataFrame\u001b[39;00m\n\u001b[1;32m     11\u001b[0m data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFactor1\u001b[39m\u001b[38;5;124m'\u001b[39m: factor1, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFactor2\u001b[39m\u001b[38;5;124m'\u001b[39m: factor2, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResponse\u001b[39m\u001b[38;5;124m'\u001b[39m: response}\n\u001b[0;32m---> 12\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(data)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Fit the two-way ANOVA model\u001b[39;00m\n\u001b[1;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m ols(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResponse ~ Factor1 + Factor2 + Factor1:Factor2\u001b[39m\u001b[38;5;124m'\u001b[39m, data\u001b[38;5;241m=\u001b[39mdf)\u001b[38;5;241m.\u001b[39mfit()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Example data (replace with your data)\n",
    "factor1 = np.array(['A', 'A', 'A', 'B', 'B', 'B'])\n",
    "factor2 = np.array(['X', 'Y', 'Z', 'X', 'Y', 'Z'])\n",
    "response = np.array([10, 12, 15, 8, 11, 9])\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {'Factor1': factor1, 'Factor2': factor2, 'Response': response}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Fit the two-way ANOVA model\n",
    "model = ols('Response ~ Factor1 + Factor2 + Factor1:Factor2', data=df).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Extract main effects and interaction effect\n",
    "main_effect_factor1 = anova_table.loc['Factor1', 'sum_sq'] / anova_table.loc['Factor1', 'df']\n",
    "main_effect_factor2 = anova_table.loc['Factor2', 'sum_sq'] / anova_table.loc['Factor2', 'df']\n",
    "interaction_effect = anova_table.loc['Factor1:Factor2', 'sum_sq'] / anova_table.loc['Factor1:Factor2', 'df']\n",
    "\n",
    "print(\"Main Effect of Factor 1:\", main_effect_factor1)\n",
    "print(\"Main Effect of Factor 2:\", main_effect_factor2)\n",
    "print(\"Interaction Effect:\", interaction_effect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9921f5e2-a83a-4c54-ad83-f47c51062e5f",
   "metadata": {},
   "source": [
    "In this scenario, since the p-value (0.02) is less than the conventional significance level of 0.05, we reject the null hypothesis. Therefore, we can conclude that there are statistically significant differences between the groups.\n",
    "\n",
    "Interpreting these results, we can say that there is evidence to suggest that at least one group mean is significantly different from the others. However, we cannot determine from the ANOVA alone which specific groups differ from each other; additional post-hoc tests or contrasts would be needed for that purpose.\n",
    "\n",
    "In summary, the obtained F-statistic of 5.23 with a p-value of 0.02 indicates that there are significant differences between the groups in the variable of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0284dc6-709e-4598-906e-27803cc58ba3",
   "metadata": {},
   "source": [
    "Handling missing data in a repeated measures ANOVA requires careful consideration, as different methods for handling missing data can lead to different results and potentially bias the conclusions drawn from the analysis. Here are some common approaches for handling missing data in repeated measures ANOVA:\n",
    "\n",
    "1. Complete Case Analysis (CCA): This approach involves excluding any cases with missing data from the analysis. While straightforward, CCA can lead to biased estimates if the missing data are not missing completely at random (MCAR) and can reduce statistical power if a large portion of the data is missing.\n",
    "\n",
    "2. Mean Imputation: Missing values are replaced with the mean of the observed values for that variable. While simple to implement, mean imputation can underestimate standard errors and bias parameter estimates, leading to incorrect inferences.\n",
    "\n",
    "3. Last Observation Carried Forward (LOCF): Missing values are replaced with the last observed value for that variable. LOCF assumes that the missing values would have followed the same trajectory as the last observed value. However, this assumption may not always hold, especially if the missing data are due to dropout or nonresponse.\n",
    "\n",
    "4. Linear Interpolation: Missing values are replaced with values interpolated from adjacent observed values. This method assumes a linear relationship between time points and may be appropriate for continuous variables with a linear trend. However, it may introduce bias if the data do not follow a linear pattern.\n",
    "\n",
    "5. Multiple Imputation: This involves generating multiple plausible values for each missing data point based on the observed data and imputing them separately. The results from each imputed dataset are then combined to obtain overall estimates and standard errors. Multiple imputation can provide unbiased estimates if the missing data mechanism is missing at random (MAR) and is often preferred when data are missing nonrandomly.\n",
    "\n",
    "The potential consequences of using different methods to handle missing data include biased parameter estimates, underestimated standard errors, inflated Type I error rates, and reduced statistical power. It's essential to carefully consider the assumptions underlying each method and choose the most appropriate approach based on the characteristics of the data and the missing data mechanism. Additionally, sensitivity analyses can help assess the robustness of the results to different methods of handling missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcd50e7-6049-4706-a5f8-f69901e12bb0",
   "metadata": {},
   "source": [
    "Common post-hoc tests used after ANOVA include:\n",
    "\n",
    "1. Tukey's Honestly Significant Difference (HSD): Tukey's HSD test is used to compare all possible pairs of group means and determine which pairs are significantly different from each other. It is suitable when you have three or more groups and want to conduct pairwise comparisons while controlling the overall Type I error rate.\n",
    "\n",
    "2. Bonferroni Correction: The Bonferroni correction adjusts the significance level for multiple comparisons to maintain the overall Type I error rate. It is suitable when conducting multiple pairwise comparisons and you want to be more conservative in controlling for false positives.\n",
    "\n",
    "3. Sidak Correction: Similar to the Bonferroni correction, the Sidak correction adjusts the significance level for multiple comparisons. It is less conservative than Bonferroni and can be used when conducting a large number of comparisons.\n",
    "\n",
    "4. Duncan's Multiple Range Test (MRT): Duncan's MRT is used to compare all possible pairs of group means, similar to Tukey's HSD test. It is less conservative than Tukey's test and can be used when you have a large number of groups.\n",
    "\n",
    "5. Holm-Bonferroni Method: The Holm-Bonferroni method is a step-down procedure that adjusts the significance level sequentially for multiple comparisons. It is suitable when you have a mix of planned and unplanned comparisons.\n",
    "\n",
    "6. Dunnett's Test: Dunnett's test compares each treatment group mean to a control group mean. It is used when there is a control group and you want to compare all other groups to the control group.\n",
    "\n",
    "7. Fisher's Least Significant Difference (LSD): Fisher's LSD test compares all possible pairs of group means. It is less conservative than Tukey's test and can be used when sample sizes are unequal or group variances are unequal.\n",
    "\n",
    "Post-hoc tests are necessary when you have rejected the null hypothesis in an ANOVA and want to determine which specific group means differ from each other. For example, suppose you conducted a one-way ANOVA to compare the effectiveness of three different teaching methods on student performance. If the ANOVA indicates that there is a significant difference between the teaching methods, you would use a post-hoc test to identify which pairs of teaching methods are significantly different from each other. This information can help inform decisions about which teaching methods are most effective and should be adopted in educational practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cc2c5c1-765d-4ec0-9636-5e7ccbfe50bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 0.05175517193522657\n",
      "P-value: 0.9495787307602778\n",
      "The one-way ANOVA result is not significant, indicating that there are no significant differences between the mean weight loss of the three diets.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Example data (replace with your data)\n",
    "weight_loss_A = np.array([2.5, 3.2, 4.0, 2.8, 3.5, 3.0, 2.0, 3.8, 2.3, 3.6,\n",
    "                          2.1, 3.9, 4.2, 3.3, 3.7, 2.9, 3.4, 3.1, 3.6, 2.7,\n",
    "                          3.5, 2.6, 3.8, 3.4, 2.2, 4.1, 3.0, 2.5, 2.9, 3.2,\n",
    "                          2.8, 3.7, 3.3, 2.4, 2.7, 3.6, 3.9, 3.2, 2.3, 3.5,\n",
    "                          4.0, 3.1, 2.6, 2.8, 3.4, 3.0, 3.8, 2.9, 2.2])\n",
    "\n",
    "weight_loss_B = np.array([3.6, 2.8, 3.5, 2.1, 3.9, 4.1, 3.0, 3.8, 2.5, 3.2,\n",
    "                          2.9, 4.0, 3.3, 2.7, 3.6, 2.2, 3.7, 2.4, 3.1, 3.4,\n",
    "                          2.6, 3.0, 2.3, 3.5, 4.2, 3.8, 3.7, 2.0, 3.9, 3.2,\n",
    "                          3.1, 2.8, 3.6, 2.7, 2.5, 3.3, 3.4, 2.6, 3.2, 2.9,\n",
    "                          3.8, 4.0, 3.7, 2.4, 3.1, 2.3, 2.7, 3.5, 3.9, 3.0])\n",
    "\n",
    "weight_loss_C = np.array([2.9, 3.8, 3.2, 4.0, 2.7, 3.5, 2.1, 3.7, 3.0, 2.5,\n",
    "                          3.6, 2.8, 4.1, 3.3, 2.4, 3.9, 3.1, 2.6, 3.4, 2.3,\n",
    "                          3.7, 2.0, 3.8, 3.2, 2.2, 3.5, 2.9, 4.0, 3.6, 2.7,\n",
    "                          3.1, 3.3, 2.8, 3.9, 2.6, 3.4, 3.0, 2.1, 3.7, 2.4,\n",
    "                          3.8, 3.2, 2.5, 3.6, 3.3, 2.8, 4.0, 2.9, 3.1, 2.7])\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = f_oneway(weight_loss_A, weight_loss_B, weight_loss_C)\n",
    "\n",
    "# Print results\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(\"P-value:\", p_value)\n",
    "\n",
    "# Interpretation\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"The one-way ANOVA result is significant, indicating that there are significant differences between the mean weight loss of the three diets.\")\n",
    "else:\n",
    "    print(\"The one-way ANOVA result is not significant, indicating that there are no significant differences between the mean weight loss of the three diets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2573f8-c16a-40d4-9f8d-1a02af3223d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
